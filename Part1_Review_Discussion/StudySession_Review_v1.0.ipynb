{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Deep Learning #\n",
    "## Part 1 : Review Session 1 ##\n",
    "——————————————————————————————————————————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1 : Generative Modelling ##\n",
    "- Generative Modelling Vs Discriminative Modelling\n",
    " - Discriminative modelling\n",
    "\n",
    "    Estimates $p(y|x)$ $-$ the probability of a label $y$ given observation $x$\n",
    " - Generative modelling\n",
    "\n",
    "    Estimates $p(x)$ $-$ the probability of observing the observation $x$. If the dataset is labeled, we can also build a generative model that estimates the distribution $p(x|y)$\n",
    "    \n",
    "- Generative Modelling Framework\n",
    " - Given some observations $(X)$, we assume that the observations have been generated from an unknown distribution $p_{data}$\n",
    " - The generative model, $p_{model}$, tries to mimic $p_{data}$. If we are able to build a good generative model, then we can sample from $p_{model}$ to generate observations that appear to have been drawn from $p_{data}$\n",
    " - Fundamental requirements for $p_{model}$\n",
    "   - Rule 1 - It can generate examples that appear to have been drawn from $p_{data}$\n",
    "   - Rule 2 - It can generate examples that are suitably different from the observations in $X$, that means, the model shouldn't simply reproduce things it has already seen (See exmaple below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./imgs/GenDeepLearn_Part1_Review_Pic1.jpg)\n",
    "\n",
    "- <b style=\"color:black\">Black dots : $p_{data}$</b>\n",
    "- <b style=\"color:red\">Red dots : Generated points</b>\n",
    "- <b>Evaluation based on above *fundamental requirements</b>\n",
    " - Point A : Breaks *Rule 1*, as it appears to have generated in the middle of the sea.\n",
    " - Point B : Breaks *Rule 2*, as it is too similar to existing data $X$\n",
    " - Point C : Seems to be good generation, satisfying all fundamental requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <h3> Probabilistic Generative Models </h3>\n",
    " Some important terminologies to be considered\n",
    " \n",
    " - Sample Space\n",
    " - Probability Distribution Function $p(x)$ (Density Function)\n",
    " - Parametric Modelling : A family of density functions, for individual paramters that define the complete data $X$\n",
    " - Likelihood\n",
    "   \n",
    "   It is the probability of observing a parameter $\\theta$ given a point $x$, represented as $L(\\theta|x) = p_{\\theta}(x)$. This is plausibility of $\\theta$ given an observed point $x$. If we have a complete data $X$, then this plausibility would become\n",
    "   \n",
    "   $L(\\theta|X) = \\prod_{x \\in X}p_{\\theta}(x)$\n",
    "   \n",
    "   Since this computationally heavy, hence we use a logarithmic version of this as\n",
    "   \n",
    "   $l(\\theta|X) = \\sum_{x \\in X}log(p_{\\theta}(x))$\n",
    "   \n",
    " - Maximum Likelihood Estimation\n",
    "   Given the above likelihood definition, there can be a set/group of $theta$ parameters most likely explain the observed data $X$. Say these parameters are $\\hat{\\theta}$. So the formulation would be\n",
    "   \n",
    "   $\\hat{\\theta} = {argmax}_{\\theta} L(\\theta | X)$\n",
    "\n",
    " With the above fundamentals, lets consider following example\n",
    "\n",
    " There are 50 people, who have stated there fashion preferences as given below. The possible values of each parameter is also given below.\n",
    " ![](./imgs/GenDeepLearn_Part1_Review_Pic2.jpg)\n",
    " \n",
    " ![](./imgs/GenDeepLearn_Part1_Review_Pic3.jpg)\n",
    " \n",
    " Given the above case, there can be $7 x 6 x 3 x 4 x 8 = 4032$ possible combinations of the above features. We are given some 50 possible combinations in $X$.\n",
    " The maximum likelihood estimate of $\\hat{\\theta}$ for each parameter would be given by\n",
    " \n",
    " $\\hat{\\theta_{j}} = \\displaystyle\\frac{n_{j}}{N}$\n",
    " \n",
    " $n_{j}$ is the number of times a combination was observed in the dataset $X$ which has a total of $N = 50$ observations\n",
    " \n",
    " <b style=\"color:red\">PROBLEM 1</b>\n",
    " \n",
    " Considering the above formulation, if a certain combination $(j)$ is never seen, the its $n_{j}$ would be zero (0) leading to maximum likelihood to be 0, and hence we would never be able to generate anything new. This is why we introduce some *pseudocount* in the calculation by changing the formula as\n",
    " \n",
    " $\\hat{\\theta_{j}} = \\displaystyle\\frac{n_{j} + 1}{N + d}$ -> N = 50 & d = 4031\n",
    " \n",
    " `\n",
    " MLT DISCUSSION\n",
    " We had some interesting QnA here. \n",
    " Q1) If we are adding 1 in the numerator, why are we adding (d = 4031) in the denominator?\n",
    " (A) We can have 4031 possible combinations, out of which we already are given 50 combinations. If we add 1 in the numerator, then we are assuming that all possible combinations have atleast 1 occurence, and hence for ratio calculation, we add 4031 in the denominator as well.\n",
    " `\n",
    " \n",
    " <b style=\"color:red\">PROBLEM 2</b>\n",
    " \n",
    " Even if we consider the above modified formulation, it is clear that the combinations which were not seen in the dataset still has same probability. Hence, the probability of an unnatural combination would also as possible as the possibility of natural combination which does not exist in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <h3> Naive Bayes based Generative Models </h3>\n",
    " In order to tackle the problems stated above, we move towards Naive assumption.\n",
    "\n",
    " ***Each feature $x_{k}$ is independent in nature, and exihibit no correlation with any other feature in the dataset $(X)$***\n",
    "\n",
    " Formally speaking\n",
    "\n",
    " $\\displaystyle p(x_{k} | x_{j}) = p(x_{k})$ --> NAIVE ASSUMPTION\n",
    "\n",
    " We know that the $p(x)$, that is, probability of observing a point $(x)$ in the space, is the joint probability of all the concerning features which define the point $(x)$. Therefore, by using conditional probability and chain rule of probability we can say\n",
    "\n",
    " $\n",
    " \\begin{aligned}\n",
    " p(x)&= p(x_1, x_2, x_3, \\dots , x_K) \\\\\n",
    "     &= p(x_2,x_3, \\dots, x_K | x_1)p(x_1) \\\\\n",
    "     &= \\prod_{k=1}^{K}p(x_k | x_1, x_2, \\dots x_{k-1})\n",
    " \\end{aligned}\n",
    " $\n",
    "\n",
    " Based on Naive Assumption, the above formulation changes to\n",
    "\n",
    " $ \n",
    " \\begin{aligned}\n",
    " p(x) &= \\prod_{k=1}^{K}p(x_k)\n",
    " \\end{aligned}\n",
    " $ \n",
    "\n",
    " <b style=\"color:red\">Explanation</b>\n",
    "\n",
    " In the **Probabilistic Approach** we were taking the probabilities based on the complete combination of features. Which in-turn increased number of possible combinations as well, due to limited observable points from the data, we did could not elaborate the model to points which were not never seen. (even with smoothing). According to **Naive Assumption**, we have removed the combination element, by assuming independence. Now, for every feature, we calculate the probabilities for each feature (based on the multi-nomial aspect of each feature), and subsequently calculate the net probabilities based on equation above.\n",
    "\n",
    " ![](./imgs/GenDeepLearn_Part1_Review_Pic4.jpg) \n",
    "\n",
    " Based on above, if we want to calculate the probability of a combination given below\n",
    "\n",
    " $\n",
    " \\begin{aligned}\n",
    " p(LongHairStraight,Red,Round,ShirtScoopNeck,White)\n",
    " \\end{aligned}\n",
    " $\n",
    "\n",
    " $\n",
    " \\begin{aligned}\n",
    " &= p(LongHairStraight) p(Red)  p(Round) p(ShirtScoopNeck) p(White)\\\\\n",
    " &= 046 x 0.16 x 0.44 x 0.38 x 0.44 \\\\\n",
    " &= 0.0054\n",
    " \\end{aligned}\n",
    " $\n",
    "\n",
    " By using this methodology, we can estimate the probability of those combinations that do not exist.\n",
    "\n",
    " <b style=\"color:red\">PROBLEM 3</b>\n",
    " \n",
    " The problem with above approach is that, we have made an assumption that individual features are indepedent. When we use a multi-nomial data then it may be able to generate new set of data points which might be possible, however, when we are looking a large featured dataset, then inter-relationship between features cannot be ignored. \n",
    " \n",
    " - How does the model cope with the high degree of conditional dependence between features?\n",
    " - How does the model find one of the tuny propotion of satisfying possible generated observations among a high-dimentional sample space?\n",
    " \n",
    "- <h3> Representation Learning </h3>\n",
    " Representation learning, articulates that instead of modelling in high-dimentional space, we represent the data in lower dimentional latent space. Thereafter, we have a mapping function, which can re-create high-demintional data from a given laten space. For eg, in order to build cylinder, we need radius and height. However, if we are given only images of cylinders, then the idea would be infer the 2 dimensions (radius and height) from these images, and if we are able to infer them, then through a mapping function, we can always create a new cylinder given any 2 combinations of radius and height.\n",
    " \n",
    " ![](./imgs/GenDeepLearn_Part1_Review_Pic5.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 : Deep Learning ##\n",
    "\n",
    "This chapter is about a fast-track introduction to neural network based models in **Keras**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 : VARIATIONAL AUTO ENCODERS (VAE) ##\n",
    "\n",
    "Variational encoders act as the fundamental building block for **Generative Adversarial Networks (GANs)**. Before, we move to VAEs, it is important o understand the building blocks of VAE, that ae encoders and decoders. The idea of encoders and decoders are aimed at building the **Representation Learning** that we discussed in the previous section. The core idea being representation of higher dimension space into lower dimension space, which is able to *represent* latent features. These representation learning aspects are built using **AUTO ENCODERS**. The fundamental building blocks of auto-encoders are:\n",
    "\n",
    " - Encoder\n",
    "   \n",
    "   An encoder network compresses high-dimensional input data into a lower dimension representation vector\n",
    "   \n",
    "   For eg: For images, we can use a standard **Conv2D** network to compress the information provided in images over multiple channels into a smaller representation at lower dimension space. Something like below\n",
    "   \n",
    "   ![](./imgs/GenDeepLearn_Part1_Review_Pic7.jpg)\n",
    "   \n",
    " - Decoder\n",
    " \n",
    "   A decoder network decompresses given representation vector back to the original domain\n",
    "   \n",
    "   For eg: Continuing on the example, above, we use **Conv2DTranspose** network to expand from a lower dimension representation back to an image. Something like below\n",
    "   \n",
    "   ![](./imgs/GenDeepLearn_Part1_Review_Pic8.jpg)\n",
    "   \n",
    "   `\n",
    "   Note on Conv2DTranspose\n",
    "   Conv2D layers impose filters which result in reduction of the input tensor (say using a stride = 2).\n",
    "   Conv2DTranspose layers impose similar filters, however, but result in increase in input tensor size (say using a stride = 2)\n",
    "   In Conv2DTranspose layer the strides parameter determines the internal zero padding between the pixels in the image.\n",
    "   `\n",
    "   \n",
    " Using the above **encoder/decoder** networks, the aim is to build something like below.\n",
    " \n",
    " ![](./imgs/GenDeepLearn_Part1_Review_Pic6.jpg)\n",
    " \n",
    " <b style=\"color:blue\"> The loss function for encoder-decoder network </b>\n",
    " \n",
    " We use a standard RMSE loss for the pixel values that the decoder predict.\n",
    " \n",
    " <b style=\"color:blue\"> MLT discussion on Convolution</b>\n",
    " \n",
    " The way convolution works is by getting filters established for every input channel (say n = 3). Suppose the output channels are (N = 32), then we would need (n X N = 3 * 32) filters in total. If we each filter is 3x3 then we have $3 x 3 x 3 x 32$ filter weights and $32 bias$, so the total parameters become $(3 x 3 x 3 x 32) + 32 = 896$ parameters\n",
    " \n",
    " ![](./imgs/GenDeepLearn_Part1_Review_Pic9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3> Analyzing an auto-encoder network </h3>\n",
    " Considering an auto-encoder used for images, and that too with MNIST dataset for numerical integers, we see that the distribution of representation looks like below. Note, that we used a representation vector length of 2, so ${x_1}$ and ${x_2}$ form the axis in the below representation diagram. Also, the colors represent the number/integer they represent\n",
    " \n",
    " ![](./imgs/GenDeepLearn_Part1_Review_Pic10.jpg)\n",
    " \n",
    " - Inferences from above representations built\n",
    "  1. The plot is not symmetrical about $(0,0)$, and not bounded too. \n",
    "  2. Some digits are represented over a very large area, and some are represented over a very small area.\n",
    "  3. There are large gaps between colors containing few mins, that means, the clusters so being formed are not that coherent or to say tightly packed.\n",
    "  \n",
    " <b style=\"color:red\"> PROBLEM 4 </b>\n",
    " - Issues seen with the above representaions\n",
    "  1. The original idea was to be able to choose a random point in the space represented and build a number out of that representation. However, the distribution is not uniform, and hence sampling from the sample space becomes problematic. Based on above, any point in that space should represent some digit, however, due to undefined behavior this ability is lost.\n",
    "  2. Due to unbounded and undefined nature of the representation, if we take any point in the space represented, it will/may not result in a good image.\n",
    "  3. When we have 2D representation, (that is smaller latent space - low dimension space), this problem is not that apparent. However, as the dimensions increase this would become even harder, because same digits would be represented in space in a manner that they would be very far off, and thus it would not be a proper laten space representation of the higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <h3> Variational Autoencoder </h3>\n",
    "  \n",
    "  We noticed the problem above in a vanilla auto-encoder system. Variational Auto encoder systems aim at addressing those issues.\n",
    "  In **auto-encoders** a single digit was mapped to a single point in the space.\n",
    "  In **Variational Auto Encoder**, a single digit is mapped to a **normal distribution** around a point in latent space.\n",
    "  \n",
    "  ![](./imgs/GenDeepLearn_Part1_Review_Pic11.jpg)\n",
    "  \n",
    "  <h4> Explanation for representation of a point over normally distributed space </h4>\n",
    "  \n",
    "  - Note on Normal Distrbution\n",
    "   - Formally\n",
    "   \n",
    "     $\\displaystyle f( x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}{\\epsilon^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}}$\n",
    "     \n",
    "     The above is normal distribution in 1 dimensional space. (given the mean $(\\mu)$ and $(\\sigma^2)$)\n",
    "     \n",
    "   - Standard Normal Distrbution\n",
    "     \n",
    "     Standard normal distrbution has $(\\mu = 0)$ & $(\\sigma^2 = 1)$\n",
    "     \n",
    "     ![](./imgs/GenDeepLearn_Part1_Review_Pic12.jpg)\n",
    "     \n",
    "     To sample any point from any of the distrbution given above, we would use the below formulation\n",
    "     \n",
    "     $ z = \\mu + \\sigma\\epsilon $. **Remember : $\\epsilon$ is sampled from a standard normal distribution. In statistics, we also call this a Z value, wherein $Z\\in[-3,+3]$.**\n",
    "     \n",
    "   - Multi-variate normal distribution\n",
    "     \n",
    "     Extending the concept of normal distribution, the formulation for having a normal distribution with $k$ parameters can be given by\n",
    "     \n",
    "     $\\displaystyle f(x_1, x_2, \\dots, x_n) = \\frac{\\epsilon^{-\\frac{1}{2}(x-\\mu)^T\\sum{}{-1}(x-\\mu)}}{\\sqrt{(2\\pi)^k\\sum}}$\n",
    "     \n",
    "     If we want to elaborate above in 2 dimensions, then the matrix for mean $\\mu$ and matrix for covariance $\\sum$ would be defined as\n",
    "     \n",
    "     $\\mu = \\begin{pmatrix} \\mu1 \\\\ \\mu2 \\end{pmatrix}$\n",
    "     \n",
    "     $\\sum = \\begin{pmatrix}\\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Relationship and explanation of multi-variate normal distribution in VAE\n",
    "     \n",
    "     VAE assume that there is no corelation between the dimensions in the latent space, and the covariance matrix above ($\\sum$) becomes a diagonal matrix. \n",
    "     \n",
    "     **This means the encoder needs to map every input to a mean vector and a variance vector**\n",
    "     \n",
    "     Caveat: We choose to map `log_var` as this can take values $(-\\infty, +\\infty)$, which is similar to that of neural network also.\n",
    "     \n",
    "     In order to relate it back to our formula\n",
    "     \n",
    "     $z = mu + sigma * epsilon$\n",
    "     \n",
    "     $sigma = e^{(\\displaystyle\\frac{log\\_var}{2})}$\n",
    "     \n",
    "     - Idea\n",
    "       \n",
    "       Earlier when we were working with encoders, a point in latent space (say for example : $(-2,2)$) represents a digit $4$, then there was no enforcement that another point (say for example $(-2.1,2.1)$) should also look similar to $4$. But by including the multi-variate variance, we are forcing the fact that points which are near to each other in latent space should be of similar kind. This results in having stronger clusters of similar looking points and therefore better representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <h3> Variational Encoders - Implementation (Encoder network) </h3>\n",
    "  \n",
    "  (Example) Model strcuture\n",
    "  \n",
    "  ![](./imgs/GenDeepLearn_Part1_Review_Pic23.jpg)\n",
    "  \n",
    "  (Example) Programatically\n",
    "  \n",
    "  ![](./imgs/GenDeepLearn_Part1_Review_Pic24.jpg)\n",
    "  \n",
    "  1. The flatten output is the network, is fed to 2 different layers, **mu** and **log_var**. These layers are standard Dense layers, which output a vector of 2.\n",
    "  2. Both these layers, feed to a **lambda** layer, which uses **mu**, **log_var** & a **epsilon** which is samples from standard normal distribution to move the point in around central point (mu) and (variance) over a normal distribution. \n",
    "  3. The resultant, is still a vector of 2, however, the dimensions in will tend to have a multi-variate normal distribution.\n",
    "  \n",
    "<h3> LOSS Function in an Variation Auto Encoders (VAE) </h3>\n",
    "  \n",
    "  - Reconstruction Loss (RMSE)\n",
    "    - Earlier we were using RMSE as the loss value, this was known as the reconstruction loss. This loss still is used in VAE.\n",
    "    \n",
    "  - KL Divergence Loss\n",
    "    - *Kullback-Leibler (KL) divergence* loss is a measure as to how much does a distribution differ from a normal distribution. KL divergence loss is 0 when $mu = 0$ and ${log\\_var} = 0$, (signifying mean = 0 and std deviation = 1), as these terms differ KL divergence loss starts to more than 0.\n",
    "    - Mathematically, KL Loss takes a close form as\n",
    "      \n",
    "      for code \n",
    "      \n",
    "      `kl_loss = -0.5 * sum(1 + log_var - mu ^ 2 - exp(log_var))`\n",
    "      \n",
    "      or for maths\n",
    "      \n",
    "      $D_{KL}[N(\\mu, \\sigma) || N(0,1)] = \\displaystyle\\frac{1}{2}\\sum(1 + \\log(\\sigma^2) - \\mu^2 - \\sigma^2)$\n",
    "  \n",
    "  - Understanding of the Loss function for VAE\n",
    "    \n",
    "    The idea of the loss function in VAE is to provide relevance to reconstruction loss as well has loss concerning the multi-variate distribution in latent space. The output from $mu$ & $log\\_var$ layers in the network are checked and loss calculated over them. This enforces following things.\n",
    "    1. The $mu$ should be becoming as close to 0 as possible. (based on KL divergence loss formula above)\n",
    "    2. The $log\\_var$ should be becoming as close to 0 as possible, and in turn the std deviation = 1 (based on KL divergence loss above).\n",
    "    3. The latent space points are also moved using these mu and sigma and epsilon (sampled from std normal distribution) so that same/similar images are represented in similar region in the latent space.\n",
    "  \n",
    "  - Code Example\n",
    "  \n",
    "    ![](./imgs/GenDeepLearn_Part1_Review_Pic25.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Analysis of VAE </h3>\n",
    "  \n",
    "  ![](./imgs/GenDeepLearn_Part1_Review_Pic26.jpg)\n",
    "  \n",
    "  Following elements are clearly visible from the above representation.\n",
    "  1. The center of the distribution is now concentrated around (0,0).\n",
    "  2. The net variation from the center is also lying between $(+/- 4)$ on both the axes\n",
    "  3. Similar numbers can be seen to be represented in similar areas, with very less overlapping.\n",
    "  4. A specific number (say number 4) can be mapped in the latent space, and it can be seen that its variations individually are also normal distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> CONCLUSION </h2>\n",
    "\n",
    "- Standard ML algorithms are discriminative in nature, with the objective as $p(y|x)$, however, generative model aim at building understanding of how the data was generated i.e. what is the distribution of the data set. Therefore understanding, $p(x)$.\n",
    "- Probabilistic Generative Models\n",
    "  - By simple methods, we will not be able to judge the probability of data samples which do not exist in the data set.\n",
    "  - By introducing a *pseudocount* also, there is no differentiation between the different kinds of sample which do not exist, as all samples which do not exist will have the same possibility/probability.\n",
    "  - Naive-Bayes based generative models, attempt to solve the above problem, by using Naive Assumption and the chain rule of probability. However, even this method is not able to work under large featured datasets and also work under a strong assumption that features are always completely independent (Naive assumption).\n",
    "- Auto Encoders\n",
    "  - Auto encoders build the concept around representation learning. The objective is to represent a large dimensional data into a lower dimension representation. The way this is achieved is by representing the higher dimensions into a lower dimension, and then reconstructing back the original data point with the help of a decoder. The loss function in this case is **RMSE**, and is also known as the reconstruction loss. The problem with these networks is that, even at lower dimensions\n",
    "    - There is no bounds to which the representation can span, every dimension (in latent space) can have $\\in(-\\infty,+\\infty)$ values.\n",
    "    - The distribution in the lower dimension space is not symmetric\n",
    "    - Similar data points, which exihibit similar nature in higher dimension, may lie very far away in latent space. Ideally they should be closer, and should form some clusters.\n",
    "- Variational Auto Encoders\n",
    "  - Attempts to build the gaps in encoder-decoder architectures, by introduction multi-variate normal distribution concepts.\n",
    "  - Explicitly introduce a `epsilon` based variation (which is sampled from a std normal distribution) for every data point's repsentation. Also force the $mu$ and $log_var$ calculated to be more closer to std normal distribution by using a special loss function **KL Divergence Loss**.\n",
    "  - Uses *Reconstruction Loss* and *KL Divergence Loss* both as the net loss function for the network, and accordingly backpropogates gradients for weight adjustment.\n",
    "  - VAE's representation of data in lower dimension is symmetric in nature.\n",
    "  - VAE's representation of complete data is also bounded in nature.\n",
    "  - VAE's representation is able to build better clusters around similar data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
